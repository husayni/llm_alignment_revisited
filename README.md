# Alignment Revisited


This is the implementation of the following academic paper: 
```
@article{gu2025alignment,
  title={Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?},
  author={Gu, Zhuojun and Wang, Quan and Han, Shuchu},
  journal={arXiv preprint arXiv:2506.00751},
  year={2025}
}  
```

Please contact [Prof. Zhuojun Gu](https://www.albany.edu/business/faculty/zhuojun-gu) or [Dr. Shuchu Han](https://shuchu.github.io/) for any questions.

---

To Run Evaluation: 

```
uv pip install -r requirements.txt

python -m evaluate --cfg.model openai/gpt-4.1
```